{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Token Counter and Chunking Test Notebook\n",
        "\n",
        "This notebook tests the token counting and chunking functionality for yellhorn-mcp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('.'))\n",
        "\n",
        "from yellhorn_mcp.token_counter import TokenCounter\n",
        "from yellhorn_mcp.llm_manager import LLMManager, ChunkingStrategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Token Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token counts for test text:\n",
            "Text length: 1550000 characters\n",
            "\n",
            "gpt-4o:\n",
            "  Tokens: 310001\n",
            "  Model limit: 128,000\n",
            "  Can fit: False\n",
            "\n",
            "gpt-4o-mini:\n",
            "  Tokens: 310001\n",
            "  Model limit: 128,000\n",
            "  Can fit: False\n",
            "\n",
            "o4-mini:\n",
            "  Tokens: 310001\n",
            "  Model limit: 65,000\n",
            "  Can fit: False\n",
            "\n",
            "gemini-2.5-pro-preview-05-06:\n",
            "  Tokens: 330001\n",
            "  Model limit: 1,048,576\n",
            "  Can fit: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize token counter\n",
        "tc = TokenCounter()\n",
        "\n",
        "# Test text\n",
        "test_text = \"\"\"This is a test sentence to count tokens. \n",
        "Let's see how many tokens this text contains for different models.\n",
        "We'll test with both OpenAI and Gemini models.\"\"\"*10000\n",
        "\n",
        "# Test different models\n",
        "models = [\"gpt-4o\", \"gpt-4o-mini\", \"o4-mini\", \"gemini-2.5-pro-preview-05-06\"]\n",
        "\n",
        "print(\"Token counts for test text:\")\n",
        "print(f\"Text length: {len(test_text)} characters\\n\")\n",
        "\n",
        "for model in models:\n",
        "    token_count = tc.count_tokens(test_text, model)\n",
        "    limit = tc.get_model_limit(model)\n",
        "    print(f\"{model}:\")\n",
        "    print(f\"  Tokens: {token_count}\")\n",
        "    print(f\"  Model limit: {limit:,}\")\n",
        "    print(f\"  Can fit: {tc.can_fit_in_context(test_text, model)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Chunking Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a longer text that needs chunking\n",
        "long_text = \"\"\"This is the first sentence of our test document. It contains multiple sentences that will be split into chunks. \n",
        "Each chunk should maintain context from the previous chunk through overlap.\n",
        "\n",
        "This is a new paragraph. Paragraphs provide natural boundaries for splitting text. \n",
        "When we split by paragraphs, we try to keep related content together.\n",
        "\n",
        "Here's another paragraph with more content. The chunking strategy should handle both sentence-level \n",
        "and paragraph-level splitting depending on the configuration. This ensures flexibility in how we process different types of content.\n",
        "\n",
        "Let's add even more content to ensure we exceed token limits. This will force the chunking mechanism to activate. \n",
        "We want to see how well it preserves context across chunk boundaries. The overlap feature is crucial for maintaining \n",
        "continuity when processing long documents.\n",
        "\"\"\" * 10  # Repeat to make it longer\n",
        "\n",
        "model = \"gpt-4o-mini\"\n",
        "max_tokens = 500  # Small limit to force chunking\n",
        "\n",
        "# Test sentence-based chunking\n",
        "print(\"\\n=== Sentence-based Chunking ===\")\n",
        "sentence_chunks = ChunkingStrategy.split_by_sentences(\n",
        "    long_text, max_tokens, tc, model, overlap_ratio=0.1\n",
        ")\n",
        "\n",
        "print(f\"Number of chunks: {len(sentence_chunks)}\")\n",
        "for i, chunk in enumerate(sentence_chunks[:3]):  # Show first 3 chunks\n",
        "    tokens = tc.count_tokens(chunk, model)\n",
        "    print(f\"\\nChunk {i+1} ({tokens} tokens):\")\n",
        "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
        "\n",
        "# Test paragraph-based chunking\n",
        "print(\"\\n\\n=== Paragraph-based Chunking ===\")\n",
        "para_chunks = ChunkingStrategy.split_by_paragraphs(\n",
        "    long_text, max_tokens, tc, model, overlap_ratio=0.1\n",
        ")\n",
        "\n",
        "print(f\"Number of chunks: {len(para_chunks)}\")\n",
        "for i, chunk in enumerate(para_chunks[:3]):  # Show first 3 chunks\n",
        "    tokens = tc.count_tokens(chunk, model)\n",
        "    print(f\"\\nChunk {i+1} ({tokens} tokens):\")\n",
        "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Edge Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test empty text\n",
        "print(\"Empty text tokens:\", tc.count_tokens(\"\", \"gpt-4o\"))\n",
        "\n",
        "# Test very long single word\n",
        "long_word = \"a\" * 10000\n",
        "print(f\"\\nVery long word ({len(long_word)} chars):\", tc.count_tokens(long_word, \"gpt-4o\"))\n",
        "\n",
        "# Test special characters and emojis\n",
        "special_text = \"Hello üëã World! üåç Special chars: @#$%^&*() \\n\\t\\r\"\n",
        "print(f\"\\nSpecial characters tokens:\", tc.count_tokens(special_text, \"gpt-4o\"))\n",
        "\n",
        "# Test unknown model (should use default)\n",
        "print(f\"\\nUnknown model limit:\", tc.get_model_limit(\"unknown-model-xyz\"))\n",
        "print(f\"Unknown model tokens:\", tc.count_tokens(\"test\", \"unknown-model-xyz\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Token Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test response token estimation\n",
        "prompts = [\n",
        "    \"Short prompt\",\n",
        "    \"This is a medium length prompt that contains more context and information for the model to process.\",\n",
        "    \"A very long prompt \" * 100\n",
        "]\n",
        "\n",
        "print(\"Response Token Estimation:\")\n",
        "for prompt in prompts:\n",
        "    prompt_tokens = tc.count_tokens(prompt, \"gpt-4o\")\n",
        "    response_estimate = tc.estimate_response_tokens(prompt, \"gpt-4o\")\n",
        "    print(f\"\\nPrompt tokens: {prompt_tokens}\")\n",
        "    print(f\"Estimated response tokens: {response_estimate}\")\n",
        "    print(f\"Total estimated: {prompt_tokens + response_estimate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Test token counting performance\n",
        "test_sizes = [100, 1000, 10000, 50000]\n",
        "print(\"Token Counting Performance:\")\n",
        "\n",
        "for size in test_sizes:\n",
        "    text = \"Hello world. \" * (size // 13)  # Approximately 'size' characters\n",
        "    \n",
        "    start = time.time()\n",
        "    tokens = tc.count_tokens(text, \"gpt-4o\")\n",
        "    duration = time.time() - start\n",
        "    \n",
        "    print(f\"\\nText size: {len(text):,} chars\")\n",
        "    print(f\"Tokens: {tokens:,}\")\n",
        "    print(f\"Time: {duration:.3f} seconds\")\n",
        "    print(f\"Speed: {len(text) / duration:,.0f} chars/second\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sravan-yellhorn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
