{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure Functionality Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/Users/sravanj/project_work/yellhorn-mcp/yellhorn-mcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sravan-yellhorn/lib/python311.zip\n",
      "/opt/anaconda3/envs/sravan-yellhorn/lib/python3.11\n",
      "/opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/lib-dynload\n",
      "\n",
      "/opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages\n",
      "/Users/sravanj/project_work/yellhorn-mcp\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellhorn_mcp.server import get_codebase_snapshot\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .yellhornignore file, using it for filtering\n",
      "Filtering codebase with 14 blacklist and 2 whitelist patterns from .yellhornignore\n",
      "Filtered from 44 to 9 files\n"
     ]
    }
   ],
   "source": [
    "repo_path = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")\n",
    "\n",
    "user_task = \"Debug MCP\"\n",
    "\n",
    "# Get file paths from codebase snapshot\n",
    "# The get_codebase_snapshot already respects .gitignore patterns by default\n",
    "# This will give us only tracked and untracked files that aren't ignored by git\n",
    "file_paths, _ = await get_codebase_snapshot(repo_path, _mode=\"paths\")\n",
    "\n",
    "if not file_paths:\n",
    "    raise YellhornMCPError(\"No files found in repository to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_directory\n",
      "\t.yellhornignore\n",
      "\tpyproject.toml\n",
      ".claude\n",
      "\tsettings.local.json\n",
      "notebooks\n",
      "\tfile_structure.ipynb\n",
      "yellhorn_mcp\n",
      "\t__init__.py\n",
      "\tcli.py\n",
      "\tlsp_utils.py\n",
      "\tserver.py\n",
      "\ttree_utils.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assume dir_chunk is your list of directory paths (e.g. ['.', 'src', 'tests', …])\n",
    "# and file_paths is the full list of files (e.g. ['app.py', 'src/main.py', 'tests/test_main.py', …])\n",
    "\n",
    "\n",
    "\n",
    "# Now you can inject `directory_tree` into your prompt\n",
    "print(directory_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze directories from filtered files\n",
    "all_dirs = set()\n",
    "for file_path in file_paths:\n",
    "    # Get all parent directories of this file\n",
    "    parts = file_path.split('/')\n",
    "    for i in range(1, len(parts)):\n",
    "        dir_path = '/'.join(parts[:i])\n",
    "        if dir_path:  # Skip empty strings\n",
    "            all_dirs.add(dir_path)\n",
    "\n",
    "# Add root directory ('.') if there are files at the root level\n",
    "if any('/' not in f for f in file_paths):\n",
    "    all_dirs.add('.')\n",
    "    \n",
    "# Sort directories for consistent output\n",
    "sorted_dirs = sorted(list(all_dirs))\n",
    "\n",
    "# Set chunk size based on reasoning mode\n",
    "chunk_size = 3000  # Process more files per chunk for file structure mode\n",
    "    \n",
    "# Calculate number of chunks needed\n",
    "total_chunks = (len(sorted_dirs) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "\n",
    "# Create chunks of directories\n",
    "dir_chunks = []\n",
    "for i in range(0, len(sorted_dirs), chunk_size):\n",
    "    dir_chunks.append(sorted_dirs[i:i + chunk_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track important directories\n",
    "all_important_dirs = set()\n",
    "\n",
    "# Helper function to process a single chunk\n",
    "async def process_chunk(chunk_idx, dir_chunk):    \n",
    "    lines = []\n",
    "    for dir_path in dir_chunk:\n",
    "        # Choose a nicer label for the root directory\n",
    "        dir_label = 'top_directory' if dir_path == '.' else dir_path\n",
    "        lines.append(dir_label)\n",
    "\n",
    "        # Gather up to 5 direct children files of this directory\n",
    "        if dir_path == '.':\n",
    "            dir_files = [f for f in file_paths if '/' not in f]\n",
    "        else:\n",
    "            prefix = dir_path.rstrip('/') + '/'\n",
    "            dir_files = [\n",
    "                f for f in file_paths \n",
    "                if f.startswith(prefix) and '/' not in f[len(prefix):]\n",
    "            ]\n",
    "        samples = dir_files[:5]\n",
    "\n",
    "        # Append each sample file under the directory, indented with a tab\n",
    "        for f in samples:\n",
    "            lines.append(f\"\\t{os.path.basename(f)}\")\n",
    "\n",
    "    # Final single representation\n",
    "    directory_tree = \"\\n\".join(lines)\n",
    "    \n",
    "    # Construct the prompt for this chunk\n",
    "    prompt = f\"\"\"You are an expert software developer tasked with analyzing a codebase structure to identify important directories for AI context.\n",
    "\n",
    "<user_task>\n",
    "{user_task}\n",
    "</user_task>\n",
    "\n",
    "Your goal is to identify the most important directories that should be included when an AI assistant analyzes this codebase for the user's task.\n",
    "\n",
    "Below is a list of directories from the codebase (chunk {chunk_idx + 1} of {total_chunks}):\n",
    "\n",
    "<directories>\n",
    "{directory_tree}\n",
    "</directories>\n",
    "\n",
    "Analyze these directories and identify the ones that:\n",
    "1. Contain core application code relevant to the user's task\n",
    "2. Likely contain important business logic\n",
    "3. Would be essential for understanding the codebase architecture\n",
    "4. Are needed to implement the requested task\n",
    "\n",
    "Ignore directories that:\n",
    "1. Contain only build artifacts or generated code\n",
    "2. Store dependencies or vendor code\n",
    "3. Contain temporary or cache files\n",
    "4. Probably aren't relevant to the user's specific task\n",
    "\n",
    "Return your analysis as a list of important directories, one per line, in this format:\n",
    "\n",
    "```context\n",
    "dir1\n",
    "dir2\n",
    "dir3\n",
    "```\n",
    "\n",
    "Don't include explanations for your choices, just return the list in the specified format.\n",
    "\"\"\"\n",
    "    print(prompt)\n",
    "    \n",
    "    # Call the appropriate AI model based on type\n",
    "    # is_openai_model = model.startswith(\"gpt-\") or model.startswith(\"o\")\n",
    "    \n",
    "    \n",
    "#     chunk_important_dirs = set()\n",
    "    \n",
    "#     try:\n",
    "#         if is_openai_model:\n",
    "#             if not openai_client:\n",
    "#                 raise YellhornMCPError(\"OpenAI client not initialized. Is OPENAI_API_KEY set?\")\n",
    "                \n",
    "#             # Convert the prompt to OpenAI messages format\n",
    "#             messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "#             # Call OpenAI API\n",
    "#             response = await openai_client.chat.completions.create(\n",
    "#                 model=model,\n",
    "#                 messages=messages,\n",
    "#             )\n",
    "            \n",
    "#             # Extract content\n",
    "#             chunk_result = response.choices[0].message.content\n",
    "#         else:\n",
    "#             if gemini_client is None:\n",
    "#                 raise YellhornMCPError(\"Gemini client not initialized. Is GEMINI_API_KEY set?\")\n",
    "            \n",
    "#             # Call Gemini API\n",
    "#             response = await gemini_client.aio.models.generate_content(model=model, contents=prompt)\n",
    "#             chunk_result = response.text\n",
    "        \n",
    "#         # Extract directory paths from the result\n",
    "#         in_context_block = False\n",
    "#         for line in chunk_result.split('\\n'):\n",
    "#             line = line.strip()\n",
    "            \n",
    "#             if line == \"```context\":\n",
    "#                 in_context_block = True\n",
    "#                 continue\n",
    "#             elif line == \"```\" and in_context_block:\n",
    "#                 in_context_block = False\n",
    "#                 continue\n",
    "            \n",
    "#             if in_context_block and line and not line.startswith('#'):\n",
    "#                 chunk_important_dirs.add(line)\n",
    "        \n",
    "#         # If we didn't find a context block, try to extract directories directly\n",
    "#         if not chunk_important_dirs and not in_context_block:\n",
    "#             for line in chunk_result.split('\\n'):\n",
    "#                 line = line.strip()\n",
    "#                 # Only add if it looks like a directory path (no spaces, existing in our list)\n",
    "#                 if line and ' ' not in line and line in dir_chunk:\n",
    "#                     chunk_important_dirs.add(line)\n",
    "        \n",
    "#         # Log the directories found\n",
    "#         dirs_str = \", \".join(sorted(list(chunk_important_dirs))[:5])\n",
    "#         if len(chunk_important_dirs) > 5:\n",
    "#             dirs_str += f\", ... ({len(chunk_important_dirs) - 5} more)\"\n",
    "        \n",
    "#         await ctx.log(\n",
    "#             level=\"info\",\n",
    "#             message=f\"Chunk {chunk_idx + 1} processed, found {len(chunk_important_dirs)} important directories: {dirs_str}\"\n",
    "#         )\n",
    "        \n",
    "#     except Exception as chunk_error:\n",
    "#         await ctx.log(\n",
    "#             level=\"error\", \n",
    "#             message=f\"Error processing chunk {chunk_idx + 1}: {str(chunk_error)} ({type(chunk_error).__name__})\"\n",
    "#         )\n",
    "#         # Continue with next chunk despite errors\n",
    "    \n",
    "#     # Return results from this chunk\n",
    "#     return chunk_important_dirs\n",
    "\n",
    "# # Use semaphore to limit concurrency to 5 parallel calls\n",
    "# semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# async def bounded_process_chunk(chunk_idx, dir_chunk):\n",
    "#     async with semaphore:\n",
    "#         return await process_chunk(chunk_idx, dir_chunk)\n",
    "\n",
    "# # If we only have one chunk, process it directly\n",
    "# if len(dir_chunks) == 1:\n",
    "#     important_dirs = await process_chunk(0, dir_chunks[0])\n",
    "#     all_important_dirs.update(important_dirs)\n",
    "# else:\n",
    "#     # Create tasks for all chunks\n",
    "#     tasks = []\n",
    "#     for chunk_idx, dir_chunk in enumerate(dir_chunks):\n",
    "#         task = asyncio.create_task(bounded_process_chunk(chunk_idx, dir_chunk))\n",
    "#         tasks.append(task)\n",
    "    \n",
    "#     # Wait for all tasks to complete and collect results\n",
    "#     await ctx.log(level=\"info\", message=f\"Waiting for {len(tasks)} parallel LLM tasks to complete\")\n",
    "#     completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "#     # Process results\n",
    "#     for result in completed_tasks:\n",
    "#         if isinstance(result, Exception):\n",
    "#             # Log the exception but continue\n",
    "#             await ctx.log(level=\"error\", message=f\"Parallel task failed: {str(result)}\")\n",
    "#             continue\n",
    "            \n",
    "#         # Update our important directories collection\n",
    "#         all_important_dirs.update(result)\n",
    "\n",
    "# # If we didn't get any important directories, include all directories\n",
    "# if not all_important_dirs:\n",
    "#     await ctx.log(\n",
    "#         level=\"warning\",\n",
    "#         message=\"No important directories identified, including all directories\"\n",
    "#     )\n",
    "#     all_important_dirs = set(sorted_dirs)\n",
    "\n",
    "# await ctx.log(\n",
    "#     level=\"info\", \n",
    "#     message=f\"Processing complete, identified {len(all_important_dirs)} important directories\"\n",
    "# )\n",
    "        \n",
    "# # Generate the final .yellhorncontext file content with comments\n",
    "# final_content = \"# Yellhorn Context File - AI context optimization\\n\"\n",
    "# final_content += f\"# Generated by yellhorn-mcp curate_context tool\\n\"\n",
    "# final_content += f\"# Based on task: {user_task}\\n\\n\"\n",
    "\n",
    "# # If we have a .yellhornignore file, include its patterns first\n",
    "# if has_ignore_file and (ignore_patterns or whitelist_patterns):\n",
    "#     final_content += \"# Patterns from .yellhornignore file\\n\"\n",
    "    \n",
    "#     # Include blacklist patterns from .yellhornignore\n",
    "#     if ignore_patterns:\n",
    "#         final_content += \"# Files and directories to exclude (blacklist)\\n\"\n",
    "#         final_content += \"\\n\".join(sorted(ignore_patterns)) + \"\\n\\n\"\n",
    "        \n",
    "#     # Include whitelist patterns from .yellhornignore  \n",
    "#     if whitelist_patterns:\n",
    "#         final_content += \"# Explicitly included patterns (whitelist)\\n\"\n",
    "#         final_content += \"\\n\".join(\"!\" + pattern for pattern in sorted(whitelist_patterns)) + \"\\n\\n\"\n",
    "\n",
    "# # Sort directories for consistent output\n",
    "# sorted_important_dirs = sorted(list(all_important_dirs))\n",
    "\n",
    "# # Add section for task-specific directory context\n",
    "# final_content += \"# Task-specific directories for AI context\\n\"\n",
    "\n",
    "# # Convert important directories to explicit include patterns (with trailing slash for directories)\n",
    "# if sorted_important_dirs:\n",
    "#     final_content += \"# Important directories to specifically include\\n\"\n",
    "#     dir_includes = []\n",
    "#     for dir_path in sorted_important_dirs:\n",
    "#         # Add trailing slash for clarity that it's a directory pattern\n",
    "#         if dir_path == '.':\n",
    "#             # Root directory is a special case\n",
    "#             dir_includes.append(\"!./\")\n",
    "#         else:\n",
    "#             dir_includes.append(f\"!{dir_path}/\")\n",
    "    \n",
    "#     final_content += \"\\n\".join(dir_includes) + \"\\n\\n\"\n",
    "\n",
    "# # Add a section recommending to blacklist everything else except the important directories\n",
    "# final_content += \"# Recommended: blacklist everything else (uncomment to enable)\\n\"\n",
    "# final_content += \"# **/*\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert software developer tasked with analyzing a codebase structure to identify important directories for AI context.\n",
      "\n",
      "<user_task>\n",
      "Debug MCP\n",
      "</user_task>\n",
      "\n",
      "Your goal is to identify the most important directories that should be included when an AI assistant analyzes this codebase for the user's task.\n",
      "\n",
      "Below is a list of directories from the codebase (chunk 1 of 1):\n",
      "\n",
      "<directories>\n",
      "top_directory\n",
      "\t.yellhornignore\n",
      "\tpyproject.toml\n",
      ".claude\n",
      "\tsettings.local.json\n",
      "notebooks\n",
      "\tfile_structure.ipynb\n",
      "yellhorn_mcp\n",
      "\t__init__.py\n",
      "\tcli.py\n",
      "\tlsp_utils.py\n",
      "\tserver.py\n",
      "\ttree_utils.py\n",
      "</directories>\n",
      "\n",
      "Analyze these directories and identify the ones that:\n",
      "1. Contain core application code relevant to the user's task\n",
      "2. Likely contain important business logic\n",
      "3. Would be essential for understanding the codebase architecture\n",
      "4. Are needed to implement the requested task\n",
      "\n",
      "Ignore directories that:\n",
      "1. Contain only build artifacts or generated code\n",
      "2. Store dependencies or vendor code\n",
      "3. Contain temporary or cache files\n",
      "4. Probably aren't relevant to the user's specific task\n",
      "\n",
      "Return your analysis as a list of important directories, one per line, in this format:\n",
      "\n",
      "```context\n",
      "dir1\n",
      "dir2\n",
      "dir3\n",
      "```\n",
      "\n",
      "Don't include explanations for your choices, just return the list in the specified format.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_chunk(\u001b[32m0\u001b[39m, dir_chunks[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mprocess_chunk\u001b[39m\u001b[34m(chunk_idx, dir_chunk)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Call the appropriate AI model based on type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m is_openai_model = \u001b[43mmodel\u001b[49m.startswith(\u001b[33m\"\u001b[39m\u001b[33mgpt-\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model.startswith(\u001b[33m\"\u001b[39m\u001b[33mo\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "await process_chunk(0, dir_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSP Prompt Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .yellhornignore file, using it for filtering\n",
      "Filtering codebase with 14 blacklist and 2 whitelist patterns from .yellhornignore\n",
      "Filtered from 45 to 9 files\n"
     ]
    }
   ],
   "source": [
    "repo_path = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")\n",
    "\n",
    "from yellhorn_mcp.lsp_utils import (\n",
    "    get_lsp_snapshot,\n",
    ")\n",
    "from yellhorn_mcp.server import format_codebase_for_prompt\n",
    "\n",
    "# Get LSP snapshot (signatures only)\n",
    "file_paths, file_contents = await get_lsp_snapshot(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yellhorn_mcp/cli.py': 'def main()  # Run the Yellhorn MCP server as a standalone command.',\n",
       " 'yellhorn_mcp/lsp_utils.py': 'def extract_python_api(file_path: Path) -> list[str]  # Extract Python API (function and class signatures with docstrings) from a file.\\ndef extract_go_api(file_path: Path) -> list[str]  # Extract Go API (function, type, interface signatures, struct fields) from a file.\\nasync def get_lsp_snapshot(repo_path: Path) -> tuple[list[str], dict[str, str]]  # Get an LSP-style snapshot of the codebase, extracting API information.\\nasync def update_snapshot_with_full_diff_files(repo_path: Path, base_ref: str, head_ref: str, file_paths: list[str], file_contents: dict[str, str]) -> tuple[list[str], dict[str, str]]  # Update an LSP snapshot with full contents of files included in a diff.',\n",
       " 'yellhorn_mcp/server.py': 'def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float | None  # Calculates the estimated cost for a model API call.\\ndef format_metrics_section(model: str, usage_metadata: Any) -> str  # Formats the completion metrics into a Markdown section.\\nclass YellhornMCPError(Exception)  # Custom exception for Yellhorn MCP server.\\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[dict[str, Any]]  # Lifespan context manager for the MCP server.\\nasync def list_resources(self, ctx: Context, resource_type: str | None) -> list[Resource]  # List resources (GitHub issues created by this tool).\\nasync def read_resource(self, ctx: Context, resource_id: str, resource_type: str | None) -> str  # Get the content of a resource (GitHub issue).\\nasync def run_git_command(repo_path: Path, command: list[str]) -> str  # Run a Git command in the repository.\\nasync def get_codebase_snapshot(repo_path: Path, _mode: str, log_function) -> tuple[list[str], dict[str, str]]  # Get a snapshot of the codebase, including file list and contents.\\nasync def format_codebase_for_prompt(file_paths: list[str], file_contents: dict[str, str]) -> str  # Format the codebase information for inclusion in the prompt.\\nasync def run_github_command(repo_path: Path, command: list[str]) -> str  # Run a GitHub CLI command in the repository.\\nasync def ensure_label_exists(repo_path: Path, label: str, description: str) -> None  # Ensure a GitHub label exists, creating it if necessary.\\nasync def add_github_issue_comment(repo_path: Path, issue_number: str, body: str) -> None  # Adds a comment to a specific GitHub issue.\\nasync def update_github_issue(repo_path: Path, issue_number: str, body: str) -> None  # Update a GitHub issue with new content.\\nasync def get_github_issue_body(repo_path: Path, issue_identifier: str) -> str  # Get the body content of a GitHub issue or PR.\\nasync def get_git_diff(repo_path: Path, base_ref: str, head_ref: str) -> str  # Get the diff content between two git references.\\nasync def get_github_pr_diff(repo_path: Path, pr_url: str) -> str  # Get the diff content of a GitHub PR.\\nasync def create_github_subissue(repo_path: Path, parent_issue_number: str, title: str, body: str, labels: list[str]) -> str  # Create a GitHub sub-issue with reference to the parent issue.\\nasync def post_github_pr_review(repo_path: Path, pr_url: str, review_content: str) -> str  # Post a review comment on a GitHub PR.\\nasync def process_workplan_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, title: str, issue_number: str, ctx: Context, detailed_description: str, debug: bool, _meta: dict[str, Any] | None) -> None  # Process workplan generation asynchronously and update the GitHub issue.\\nasync def get_default_branch(repo_path: Path) -> str  # Determine the default branch name of the repository.\\ndef is_git_repository(path: Path) -> bool  # Check if a path is a Git repository.\\nasync def create_workplan(title: str, detailed_description: str, ctx: Context, codebase_reasoning: str, debug: bool) -> str  # Create a workplan based on the provided title and detailed description.\\nasync def get_workplan(ctx: Context, issue_number: str) -> str  # Retrieve the workplan content (GitHub issue body) associated with a workplan.\\nasync def process_judgement_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, workplan: str, diff: str, base_ref: str, head_ref: str, workplan_issue_number: str | None, ctx: Context, base_commit_hash: str | None, head_commit_hash: str | None, debug: bool, _meta: dict[str, Any] | None) -> str  # Process the judgement of a workplan and diff asynchronously, creating a GitHub sub-issue.\\nasync def curate_context(ctx: Context, user_task: str, codebase_reasoning: str, ignore_file_path: str, output_path: str, depth_limit: int) -> str  # Analyzes codebase and creates a .yellhorncontext file listing directories for AI context.\\nasync def judge_workplan(ctx: Context, issue_number: str, base_ref: str, head_ref: str, codebase_reasoning: str, debug: bool) -> str  # Trigger an asynchronous code judgement comparing two git refs against a workplan.',\n",
       " 'yellhorn_mcp/tree_utils.py': 'def build_tree(file_paths: list[str], max_depth: int, max_files: int) -> str  # Build a tree representation of files in the repository.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "async def format_codebase_for_prompt(\n",
    "    file_paths: List[str],\n",
    "    file_contents: Dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Format the codebase information for inclusion in the prompt.\n",
    "\n",
    "    Args:\n",
    "        file_paths: List of file paths.\n",
    "        file_contents: Dictionary mapping file paths to contents.\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with codebase tree and inlined file contents.\n",
    "    \"\"\"\n",
    "    # 1. Gather unique directories (including root as '.')\n",
    "    dirs = set(Path(fp).parent.as_posix() for fp in file_paths)\n",
    "    # ensure root appears\n",
    "    dirs.add('.')\n",
    "    # sort so root comes first, then lexicographically\n",
    "    dir_list = sorted(dirs, key=lambda d: (d != '.', d))\n",
    "\n",
    "    lines: List[str] = []\n",
    "    for dir_path in dir_list:\n",
    "        # pretty label\n",
    "        label = 'top_directory' if dir_path == '.' else dir_path\n",
    "        lines.append(label)\n",
    "\n",
    "        # find files directly in this directory\n",
    "        if dir_path == '.':\n",
    "            dir_files = [f for f in file_paths if '/' not in f]\n",
    "        else:\n",
    "            prefix = dir_path.rstrip('/') + '/'\n",
    "            dir_files = [\n",
    "                f for f in file_paths\n",
    "                if f.startswith(prefix) and '/' not in f[len(prefix):]\n",
    "            ]\n",
    "\n",
    "        for fp in sorted(dir_files):\n",
    "            name = os.path.basename(fp)\n",
    "            lines.append(f\"\\t{name}\")\n",
    "            # inline the file’s contents right after its name\n",
    "            content = file_contents.get(fp, \"\").rstrip()\n",
    "            if content:\n",
    "                # decide syntax highlighting by extension\n",
    "                ext = Path(fp).suffix.lstrip('.')\n",
    "                lang = ext or 'text'\n",
    "                # indent each line of content by one more tab\n",
    "                indented = \"\\n\".join(\"\\t\\t\" + l for l in content.splitlines())\n",
    "                lines.append(f\"\\t\\t```{lang}\\n{indented}\\n\\t\\t```\")\n",
    "\n",
    "    codebase_contents = \"\\n\".join(lines)\n",
    "    return f\"\"\"<codebase_tree>\n",
    "{codebase_contents}\n",
    "</codebase_tree>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<codebase_tree>\n",
      "top_directory\n",
      "\t.yellhornignore\n",
      "\tpyproject.toml\n",
      ".claude\n",
      "\tsettings.local.json\n",
      "notebooks\n",
      "\tfile_structure.ipynb\n",
      "yellhorn_mcp\n",
      "\t__init__.py\n",
      "\tcli.py\n",
      "\t\t```py\n",
      "\t\tdef main()  # Run the Yellhorn MCP server as a standalone command.\n",
      "\t\t```\n",
      "\tlsp_utils.py\n",
      "\t\t```py\n",
      "\t\tdef extract_python_api(file_path: Path) -> list[str]  # Extract Python API (function and class signatures with docstrings) from a file.\n",
      "\t\tdef extract_go_api(file_path: Path) -> list[str]  # Extract Go API (function, type, interface signatures, struct fields) from a file.\n",
      "\t\tasync def get_lsp_snapshot(repo_path: Path) -> tuple[list[str], dict[str, str]]  # Get an LSP-style snapshot of the codebase, extracting API information.\n",
      "\t\tasync def update_snapshot_with_full_diff_files(repo_path: Path, base_ref: str, head_ref: str, file_paths: list[str], file_contents: dict[str, str]) -> tuple[list[str], dict[str, str]]  # Update an LSP snapshot with full contents of files included in a diff.\n",
      "\t\t```\n",
      "\tserver.py\n",
      "\t\t```py\n",
      "\t\tdef calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float | None  # Calculates the estimated cost for a model API call.\n",
      "\t\tdef format_metrics_section(model: str, usage_metadata: Any) -> str  # Formats the completion metrics into a Markdown section.\n",
      "\t\tclass YellhornMCPError(Exception)  # Custom exception for Yellhorn MCP server.\n",
      "\t\tasync def app_lifespan(server: FastMCP) -> AsyncIterator[dict[str, Any]]  # Lifespan context manager for the MCP server.\n",
      "\t\tasync def list_resources(self, ctx: Context, resource_type: str | None) -> list[Resource]  # List resources (GitHub issues created by this tool).\n",
      "\t\tasync def read_resource(self, ctx: Context, resource_id: str, resource_type: str | None) -> str  # Get the content of a resource (GitHub issue).\n",
      "\t\tasync def run_git_command(repo_path: Path, command: list[str]) -> str  # Run a Git command in the repository.\n",
      "\t\tasync def get_codebase_snapshot(repo_path: Path, _mode: str, log_function) -> tuple[list[str], dict[str, str]]  # Get a snapshot of the codebase, including file list and contents.\n",
      "\t\tasync def format_codebase_for_prompt(file_paths: list[str], file_contents: dict[str, str]) -> str  # Format the codebase information for inclusion in the prompt.\n",
      "\t\tasync def run_github_command(repo_path: Path, command: list[str]) -> str  # Run a GitHub CLI command in the repository.\n",
      "\t\tasync def ensure_label_exists(repo_path: Path, label: str, description: str) -> None  # Ensure a GitHub label exists, creating it if necessary.\n",
      "\t\tasync def add_github_issue_comment(repo_path: Path, issue_number: str, body: str) -> None  # Adds a comment to a specific GitHub issue.\n",
      "\t\tasync def update_github_issue(repo_path: Path, issue_number: str, body: str) -> None  # Update a GitHub issue with new content.\n",
      "\t\tasync def get_github_issue_body(repo_path: Path, issue_identifier: str) -> str  # Get the body content of a GitHub issue or PR.\n",
      "\t\tasync def get_git_diff(repo_path: Path, base_ref: str, head_ref: str) -> str  # Get the diff content between two git references.\n",
      "\t\tasync def get_github_pr_diff(repo_path: Path, pr_url: str) -> str  # Get the diff content of a GitHub PR.\n",
      "\t\tasync def create_github_subissue(repo_path: Path, parent_issue_number: str, title: str, body: str, labels: list[str]) -> str  # Create a GitHub sub-issue with reference to the parent issue.\n",
      "\t\tasync def post_github_pr_review(repo_path: Path, pr_url: str, review_content: str) -> str  # Post a review comment on a GitHub PR.\n",
      "\t\tasync def process_workplan_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, title: str, issue_number: str, ctx: Context, detailed_description: str, debug: bool, _meta: dict[str, Any] | None) -> None  # Process workplan generation asynchronously and update the GitHub issue.\n",
      "\t\tasync def get_default_branch(repo_path: Path) -> str  # Determine the default branch name of the repository.\n",
      "\t\tdef is_git_repository(path: Path) -> bool  # Check if a path is a Git repository.\n",
      "\t\tasync def create_workplan(title: str, detailed_description: str, ctx: Context, codebase_reasoning: str, debug: bool) -> str  # Create a workplan based on the provided title and detailed description.\n",
      "\t\tasync def get_workplan(ctx: Context, issue_number: str) -> str  # Retrieve the workplan content (GitHub issue body) associated with a workplan.\n",
      "\t\tasync def process_judgement_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, workplan: str, diff: str, base_ref: str, head_ref: str, workplan_issue_number: str | None, ctx: Context, base_commit_hash: str | None, head_commit_hash: str | None, debug: bool, _meta: dict[str, Any] | None) -> str  # Process the judgement of a workplan and diff asynchronously, creating a GitHub sub-issue.\n",
      "\t\tasync def curate_context(ctx: Context, user_task: str, codebase_reasoning: str, ignore_file_path: str, output_path: str, depth_limit: int) -> str  # Analyzes codebase and creates a .yellhorncontext file listing directories for AI context.\n",
      "\t\tasync def judge_workplan(ctx: Context, issue_number: str, base_ref: str, head_ref: str, codebase_reasoning: str, debug: bool) -> str  # Trigger an asynchronous code judgement comparing two git refs against a workplan.\n",
      "\t\t```\n",
      "\ttree_utils.py\n",
      "\t\t```py\n",
      "\t\tdef build_tree(file_paths: list[str], max_depth: int, max_files: int) -> str  # Build a tree representation of files in the repository.\n",
      "\t\t```\n",
      "</codebase_tree>\n"
     ]
    }
   ],
   "source": [
    "codebase_info = await format_codebase_for_prompt(file_paths, file_contents)\n",
    "print(codebase_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .yellhornignore file, using it for filtering\n",
      "Filtering codebase with 14 blacklist and 2 whitelist patterns from .yellhornignore\n",
      "Filtered from 42 to 8 files\n"
     ]
    }
   ],
   "source": [
    "from yellhorn_mcp.lsp_utils import get_lsp_snapshot\n",
    "from yellhorn_mcp.server import format_codebase_for_prompt\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")\n",
    "\n",
    "file_paths, file_contents = await get_lsp_snapshot(repo_path)\n",
    "# For lsp mode, format with tree and LSP file contents\n",
    "codebase_info = await format_codebase_for_prompt(file_paths, file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<codebase_tree>\n",
      "top_directory\n",
      "\t.yellhornignore\n",
      "\tpyproject.toml\n",
      ".claude\n",
      "\tsettings.local.json\n",
      "notebooks\n",
      "\tfile_structure.ipynb\n",
      "yellhorn_mcp\n",
      "\t__init__.py\n",
      "\tcli.py\n",
      "\t\t```py\n",
      "\t\tdef main()  # Run the Yellhorn MCP server as a standalone command.\n",
      "\t\t```\n",
      "\tlsp_utils.py\n",
      "\t\t```py\n",
      "\t\tdef extract_python_api(file_path: Path) -> list[str]  # Extract Python API (function and class signatures with docstrings) from a file.\n",
      "\t\tdef extract_go_api(file_path: Path) -> list[str]  # Extract Go API (function, type, interface signatures, struct fields) from a file.\n",
      "\t\tasync def get_lsp_snapshot(repo_path: Path) -> tuple[list[str], dict[str, str]]  # Get an LSP-style snapshot of the codebase, extracting API information.\n",
      "\t\tasync def update_snapshot_with_full_diff_files(repo_path: Path, base_ref: str, head_ref: str, file_paths: list[str], file_contents: dict[str, str]) -> tuple[list[str], dict[str, str]]  # Update an LSP snapshot with full contents of files included in a diff.\n",
      "\t\t```\n",
      "\tserver.py\n",
      "\t\t```py\n",
      "\t\tdef calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float | None  # Calculates the estimated cost for a model API call.\n",
      "\t\tdef format_metrics_section(model: str, usage_metadata: Any) -> str  # Formats the completion metrics into a Markdown section.\n",
      "\t\tclass YellhornMCPError(Exception)  # Custom exception for Yellhorn MCP server.\n",
      "\t\tasync def app_lifespan(server: FastMCP) -> AsyncIterator[dict[str, Any]]  # Lifespan context manager for the MCP server.\n",
      "\t\tasync def list_resources(self, ctx: Context, resource_type: str | None) -> list[Resource]  # List resources (GitHub issues created by this tool).\n",
      "\t\tasync def read_resource(self, ctx: Context, resource_id: str, resource_type: str | None) -> str  # Get the content of a resource (GitHub issue).\n",
      "\t\tasync def run_git_command(repo_path: Path, command: list[str]) -> str  # Run a Git command in the repository.\n",
      "\t\tasync def get_codebase_snapshot(repo_path: Path, _mode: str, log_function) -> tuple[list[str], dict[str, str]]  # Get a snapshot of the codebase, including file list and contents.\n",
      "\t\tdef build_file_structure_context(file_paths: list[str]) -> str  # Build a codebase info string containing only the file structure.\n",
      "\t\tasync def format_codebase_for_prompt(file_paths: List[str], file_contents: Dict[str, str]) -> str  # Format the codebase information for inclusion in the prompt.\n",
      "\t\tasync def run_github_command(repo_path: Path, command: list[str]) -> str  # Run a GitHub CLI command in the repository.\n",
      "\t\tasync def ensure_label_exists(repo_path: Path, label: str, description: str) -> None  # Ensure a GitHub label exists, creating it if necessary.\n",
      "\t\tasync def add_github_issue_comment(repo_path: Path, issue_number: str, body: str) -> None  # Adds a comment to a specific GitHub issue.\n",
      "\t\tasync def update_github_issue(repo_path: Path, issue_number: str, body: str) -> None  # Update a GitHub issue with new content.\n",
      "\t\tasync def get_github_issue_body(repo_path: Path, issue_identifier: str) -> str  # Get the body content of a GitHub issue or PR.\n",
      "\t\tasync def get_git_diff(repo_path: Path, base_ref: str, head_ref: str) -> str  # Get the diff content between two git references.\n",
      "\t\tasync def get_github_pr_diff(repo_path: Path, pr_url: str) -> str  # Get the diff content of a GitHub PR.\n",
      "\t\tasync def create_github_subissue(repo_path: Path, parent_issue_number: str, title: str, body: str, labels: list[str]) -> str  # Create a GitHub sub-issue with reference to the parent issue.\n",
      "\t\tasync def post_github_pr_review(repo_path: Path, pr_url: str, review_content: str) -> str  # Post a review comment on a GitHub PR.\n",
      "\t\tasync def process_workplan_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, title: str, issue_number: str, ctx: Context, detailed_description: str, debug: bool, _meta: dict[str, Any] | None) -> None  # Process workplan generation asynchronously and update the GitHub issue.\n",
      "\t\tasync def get_default_branch(repo_path: Path) -> str  # Determine the default branch name of the repository.\n",
      "\t\tdef is_git_repository(path: Path) -> bool  # Check if a path is a Git repository.\n",
      "\t\tasync def create_workplan(title: str, detailed_description: str, ctx: Context, codebase_reasoning: str, debug: bool) -> str  # Create a workplan based on the provided title and detailed description.\n",
      "\t\tasync def get_workplan(ctx: Context, issue_number: str) -> str  # Retrieve the workplan content (GitHub issue body) associated with a workplan.\n",
      "\t\tasync def process_judgement_async(repo_path: Path, gemini_client: genai.Client | None, openai_client: Any | None, model: str, workplan: str, diff: str, base_ref: str, head_ref: str, workplan_issue_number: str | None, ctx: Context, base_commit_hash: str | None, head_commit_hash: str | None, debug: bool, _meta: dict[str, Any] | None) -> str  # Process the judgement of a workplan and diff asynchronously, creating a GitHub sub-issue.\n",
      "\t\tasync def curate_context(ctx: Context, user_task: str, codebase_reasoning: str, ignore_file_path: str, output_path: str, depth_limit: int) -> str  # Analyzes codebase and creates a .yellhorncontext file listing directories for AI context.\n",
      "\t\tasync def judge_workplan(ctx: Context, issue_number: str, base_ref: str, head_ref: str, codebase_reasoning: str, debug: bool) -> str  # Trigger an asynchronous code judgement comparing two git refs against a workplan.\n",
      "\t\t```\n",
      "</codebase_tree>\n"
     ]
    }
   ],
   "source": [
    "print(codebase_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the File Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellhorn_mcp.server import run_git_command\n",
    "# import fnmatch\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_codebase_snapshot(repo_path: Path, _mode: str = \"full\", log_function = print) -> tuple[list[str], dict[str, str]]:\n",
    "    # Get list of all tracked and untracked files (respects .gitignore by default)\n",
    "    files_output = await run_git_command(repo_path, [\"ls-files\", \"-c\", \"-o\", \"--exclude-standard\"])\n",
    "    file_paths = [f for f in files_output.split(\"\\n\") if f]\n",
    "\n",
    "    # Priority order: .yellhorncontext overrides .yellhornignore\n",
    "    yellhorncontext_path = repo_path / \".yellhorncontext\"\n",
    "    context_exists = yellhorncontext_path.exists() and yellhorncontext_path.is_file()\n",
    "\n",
    "    yellhornignore_path = repo_path / \".yellhornignore\"\n",
    "    ignore_exists = yellhornignore_path.exists() and yellhornignore_path.is_file()\n",
    "\n",
    "    # Initialize pattern lists\n",
    "    ignore_patterns = []\n",
    "    whitelist_patterns = []\n",
    "\n",
    "    # First try .yellhorncontext, then fall back to .yellhornignore\n",
    "    if context_exists:\n",
    "        # Read patterns from .yellhorncontext\n",
    "        with open(yellhorncontext_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                pattern = line.strip()\n",
    "                if pattern:\n",
    "                    if pattern.startswith(\"!\"):\n",
    "                        whitelist_patterns.append(pattern[1:])\n",
    "                    else:\n",
    "                        ignore_patterns.append(pattern)\n",
    "    elif ignore_exists:\n",
    "        # Read patterns from .yellhornignore\n",
    "        with open(yellhornignore_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                pattern = line.strip()\n",
    "                if pattern:\n",
    "                    ignore_patterns.append(pattern)\n",
    "\n",
    "    print(ignore_patterns)\n",
    "    # Apply filtering with fnmatch\n",
    "    if ignore_patterns or whitelist_patterns:\n",
    "        def is_ignored(file_path: str) -> bool:\n",
    "            # Check whitelist patterns first (take precedence)\n",
    "            for pattern in whitelist_patterns:\n",
    "                if fnmatch.fnmatch(file_path, pattern):\n",
    "                    return False  # Don't ignore whitelisted files\n",
    "\n",
    "            # Then check blacklist patterns\n",
    "            for pattern in ignore_patterns:\n",
    "                if fnmatch.fnmatch(file_path, pattern):\n",
    "                    return True  # Ignore matching files\n",
    "            return False\n",
    "\n",
    "        # Filter files\n",
    "        filtered_paths = [f for f in file_paths if not is_ignored(f)]\n",
    "        file_paths = filtered_paths\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Yellhorn Ignore File - AI-optimized patterns', '# Generated by yellhorn-mcp curate_ignore_file tool', '# Files and directories to exclude from AI context', '.github/', '.gitignore', '.mcp.json', '.python-version', '.vscode/', 'CHANGELOG.md', 'CLAUDE.md', 'README.md', 'assets/', 'coverage_stats.txt', 'docs/', 'examples/', 'tests/', 'tmp_repo/', '# Important files to explicitly include despite matching blacklist patterns', '!pyproject.toml', '!yellhorn_mcp/']\n",
      "['.claude/settings.local.json', '.yellhornignore', 'examples/lsp_workplan_example.py', 'notebooks/file_structure.ipynb', '.github/workflows/publish.yml', '.github/workflows/tests.yml', '.vscode/mcp.json', 'assets/yellhorn.png', 'docs/USAGE.md', 'docs/coverage_baseline.md', 'examples/__init__.py', 'examples/client_example.py', 'pyproject.toml', 'tests/__init__.py', 'tests/conftest.py', 'tests/helpers.py', 'tests/test_async_flows_openai.py', 'tests/test_cli.py', 'tests/test_cli_errors.py', 'tests/test_cost_metrics.py', 'tests/test_error_handling.py', 'tests/test_lsp_e2e.py', 'tests/test_lsp_fence_handling.py', 'tests/test_lsp_go.py', 'tests/test_lsp_mode.py', 'tests/test_lsp_python_props.py', 'tests/test_mcp_annotations.py', 'tests/test_openai.py', 'tests/test_resources_api.py', 'tests/test_server.py', 'tests/test_server_yellhornignore.py', 'tests/test_tree_utils.py', 'tmp_repo/.yellhornignore', 'yellhorn_mcp/__init__.py', 'yellhorn_mcp/cli.py', 'yellhorn_mcp/lsp_utils.py', 'yellhorn_mcp/server.py', 'yellhorn_mcp/tree_utils.py']\n"
     ]
    }
   ],
   "source": [
    "file_paths = await get_codebase_snapshot(repo_path)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'hello/.github/workflows/publish.yml'\n",
    "pat = '*.github/'\n",
    "fnmatch.fnmatch(path, pat.rstrip(\"/\") + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "whitelist_patterns = [\"python/**/*.py\", \"*.py\", \"*.ipynb\", \"*.md\"]\n",
    "ignore_patterns = [\".gitignore\", \".yellhornignore\", \"hello/*\"]\n",
    "\n",
    "# Use the same is_ignored function that get_codebase_snapshot uses\n",
    "def is_ignored(file_path: str) -> bool:\n",
    "    # First check if the file is whitelisted\n",
    "    for pattern in whitelist_patterns:\n",
    "        # Regular pattern matching (e.g., \"*.py\")\n",
    "        if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(file_path, pattern.rstrip(\"/\") + \"/*\"):\n",
    "            return False  # Whitelisted, don't ignore\n",
    "    \n",
    "    # Then check if it matches any ignore patterns\n",
    "    for pattern in ignore_patterns:\n",
    "        # Regular pattern matching (e.g., \"*.log\")\n",
    "        if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(file_path, pattern.rstrip(\"/\") + \"/*\"):\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnmatch.fnmatch(\"hello/hello/poetry.lock\", \"poetry.lock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_ignored(\"hello/hello/hello.js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sravan-yellhorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
